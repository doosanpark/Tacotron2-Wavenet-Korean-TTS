
-----------------------------------------------------------------
Starting new training run
-----------------------------------------------------------------
[2025-11-02 18:45:07.579]    ==================================================
[2025-11-02 18:45:07.579]    ==================================================
[2025-11-02 18:45:07.579]     [*] Checkpoint path: logdir-tacotron2\moon+son_2025-11-02_18-45-07\model.ckpt
[2025-11-02 18:45:07.579]     [*] Loading training data from: ['.\\data\\moon', '.\\data\\son']
[2025-11-02 18:45:07.579]     [*] Using model: logdir-tacotron2\moon+son_2025-11-02_18-45-07
[2025-11-02 18:45:07.579]    Hyperparameters:
  adam_beta1: 0.9
  adam_beta2: 0.999
  allow_clipping_in_normalization: True
  attention_dim: 128
  attention_filters: 32
  attention_kernel: (31,)
  attention_size: 128
  attention_type: bah_mon_norm
  attention_win_size: 7
  cleaners: korean_cleaners
  clip_mels_length: True
  cumulative_weights: True
  dec_prenet_sizes: [256, 256]
  decoder_layers: 2
  decoder_lstm_units: 1024
  dilation_channels: 256
  dilations: [1, 2, 4, 8, 16, 32, 64, 128, 256, 512, 1, 2, 4, 8, 16, 32, 64, 128, 256, 512]
  dropout_prob: 0.5
  embedding_size: 512
  enc_conv_channels: 512
  enc_conv_kernel_size: 5
  enc_conv_num_layers: 3
  encoder_lstm_units: 256
  fft_size: 2048
  filter_width: 3
  gc_channels: 32
  griffin_lim_iters: 60
  hop_size: 300
  inference_prenet_dropout: True
  initial_data_greedy: True
  initial_phase_step: 8000
  input_type: raw
  l2_regularization_strength: 0
  legacy: True
  main_data: ['']
  main_data_greedy_factor: 0
  mask_encoder: True
  max_abs_value: 4.0
  max_checkpoints: 3
  max_mel_frames: 1000
  max_n_frame: 1000
  min_level_db: -100
  min_n_frame: 150
  min_tokens: 30
  model_type: multi-speaker
  momentum: 0.9
  name: Tacotron-2
  num_mels: 80
  num_steps: 1000000
  optimizer: adam
  out_channels: 30
  post_bank_channel_size: 128
  post_bank_size: 8
  post_highway_depth: 4
  post_maxpool_width: 2
  post_proj_sizes: [256, 80]
  post_proj_width: 3
  post_rnn_size: 128
  postnet_channels: 512
  postnet_kernel_size: (5,)
  postnet_num_layers: 5
  power: 1.5
  preemphasis: 0.97
  preemphasize: True
  prenet_layers: [256, 256]
  prioritize_loss: False
  quantization_channels: 256
  reduction_factor: 2
  ref_level_db: 20
  rescaling: True
  rescaling_max: 0.999
  residual_channels: 128
  residual_legacy: True
  sample_rate: 24000
  sample_size: 9000
  scalar_input: True
  signal_normalization: True
  silence_threshold: 0
  skip_channels: 128
  skip_inadequate: False
  skip_path_filter: False
  smoothing: False
  speaker_embedding_size: 16
  store_metadata: False
  symmetric_mels: True
  synthesis_constraint: False
  synthesis_constraint_type: window
  tacotron_decay_learning_rate: True
  tacotron_decay_rate: 0.5
  tacotron_decay_steps: 18000
  tacotron_final_learning_rate: 0.0001
  tacotron_initial_learning_rate: 0.001
  tacotron_reg_weight: 1e-06
  tacotron_start_decay: 40000
  tacotron_zoneout_rate: 0.1
  trim_fft_size: 512
  trim_hop_size: 128
  trim_silence: True
  trim_top_db: 23
  upsample_factor: [12, 25]
  upsample_type: SubPixel
  use_biases: True
  use_lws: False
  wavenet_batch_size: 2
  wavenet_clip_gradients: True
  wavenet_decay_rate: 0.5
  wavenet_decay_steps: 300000
  wavenet_dropout: 0.05
  wavenet_learning_rate: 0.001
  win_size: 1200
[2025-11-02 18:45:12.612]     [.\data\moon] Loaded metadata for 34 examples (0.02 hours)
[2025-11-02 18:45:12.612]     [.\data\moon] Max length: 340
[2025-11-02 18:45:12.612]     [.\data\moon] Min length: 152
[2025-11-02 18:45:18.123]     [.\data\son] Loaded metadata for 38 examples (0.05 hours)
[2025-11-02 18:45:18.123]     [.\data\son] Max length: 827
[2025-11-02 18:45:18.123]     [.\data\son] Min length: 176
[2025-11-02 18:45:18.123]    ========================================
[2025-11-02 18:45:18.123]    Data Amount:
[2025-11-02 18:45:18.123]    {'.\\data\\moon': 0.5, '.\\data\\son': 0.5}
[2025-11-02 18:45:18.123]    ========================================
[2025-11-02 18:45:23.549]     [.\data\moon] Loaded metadata for 34 examples (0.02 hours)
[2025-11-02 18:45:23.549]     [.\data\moon] Max length: 340
[2025-11-02 18:45:23.549]     [.\data\moon] Min length: 152
[2025-11-02 18:45:29.068]     [.\data\son] Loaded metadata for 38 examples (0.05 hours)
[2025-11-02 18:45:29.068]     [.\data\son] Max length: 827
[2025-11-02 18:45:29.068]     [.\data\son] Min length: 176
[2025-11-02 18:45:29.068]    ========================================
[2025-11-02 18:45:29.068]    Data Amount:
[2025-11-02 18:45:29.068]    {'.\\data\\moon': 0.5, '.\\data\\son': 0.5}
[2025-11-02 18:45:29.068]    ========================================
[2025-11-02 18:45:30.338]    ========================================
[2025-11-02 18:45:30.338]     model_type: multi-speaker
[2025-11-02 18:45:30.338]    ========================================
[2025-11-02 18:45:30.338]    Initialized Tacotron model. Dimensions: 
[2025-11-02 18:45:30.338]        embedding:                512
[2025-11-02 18:45:30.338]        encoder conv out:               512
[2025-11-02 18:45:30.338]        encoder out:              512
[2025-11-02 18:45:30.338]        attention out:            1024
[2025-11-02 18:45:30.338]        decoder prenet lstm concat out :        1536
[2025-11-02 18:45:30.338]        decoder cell out:         162
[2025-11-02 18:45:30.338]        decoder out (2 frames):  162
[2025-11-02 18:45:30.338]        decoder mel out:    80
[2025-11-02 18:45:30.338]        mel out:    80
[2025-11-02 18:45:30.338]        postnet out:              256
[2025-11-02 18:45:30.338]        linear out:               1025
[2025-11-02 18:45:30.339]      Tacotron Parameters       29.229 Million.
[2025-11-02 18:45:34.010]    ========================================
[2025-11-02 18:45:34.010]     model_type: multi-speaker
[2025-11-02 18:45:34.010]    ========================================
[2025-11-02 18:45:34.010]    Initialized Tacotron model. Dimensions: 
[2025-11-02 18:45:34.010]        embedding:                512
[2025-11-02 18:45:34.010]        encoder conv out:               512
[2025-11-02 18:45:34.010]        encoder out:              512
[2025-11-02 18:45:34.010]        attention out:            1024
[2025-11-02 18:45:34.010]        decoder prenet lstm concat out :        1536
[2025-11-02 18:45:34.010]        decoder cell out:         162
[2025-11-02 18:45:34.010]        decoder out (2 frames):  162
[2025-11-02 18:45:34.010]        decoder mel out:    80
[2025-11-02 18:45:34.010]        mel out:    80
[2025-11-02 18:45:34.010]        postnet out:              256
[2025-11-02 18:45:34.010]        linear out:               1025
[2025-11-02 18:45:34.011]      Tacotron Parameters       29.229 Million.
[2025-11-02 18:45:36.827]    Starting new training run at commit: None
[2025-11-02 18:45:37.065]    Generated 8 batches of size 4 in 0.000 sec
[2025-11-02 18:45:40.098]    Generated 32 batches of size 32 in 3.034 sec
[2025-11-02 18:45:49.677]    Step 1       [12.612 sec/step, loss=7.19503, avg_loss=7.19503]
[2025-11-02 18:46:06.337]    Step 2       [14.636 sec/step, loss=5.99218, avg_loss=6.59360]
[2025-11-02 18:46:15.250]    Step 3       [12.728 sec/step, loss=5.91748, avg_loss=6.36823]
[2025-11-02 18:46:49.033]    Step 4       [17.992 sec/step, loss=7.05303, avg_loss=6.53943]
[2025-11-02 18:47:07.743]    Step 5       [18.136 sec/step, loss=6.10396, avg_loss=6.45234]
[2025-11-02 18:47:16.862]    Step 6       [16.633 sec/step, loss=4.69992, avg_loss=6.16027]
[2025-11-02 18:47:25.941]    Step 7       [15.554 sec/step, loss=4.24343, avg_loss=5.88643]
[2025-11-02 18:47:42.948]    Step 8       [15.735 sec/step, loss=4.64419, avg_loss=5.73115]
[2025-11-02 18:47:50.905]    Step 9       [14.871 sec/step, loss=5.57770, avg_loss=5.71410]
[2025-11-02 18:48:26.023]    Step 10      [16.896 sec/step, loss=4.16648, avg_loss=5.55934]
[2025-11-02 18:48:35.716]    Step 11      [16.241 sec/step, loss=3.76553, avg_loss=5.39627]
[2025-11-02 18:48:55.040]    Step 12      [16.498 sec/step, loss=3.84452, avg_loss=5.26695]
[2025-11-02 18:49:04.464]    Step 13      [15.954 sec/step, loss=3.35567, avg_loss=5.11993]
[2025-11-02 18:49:13.565]    Step 14      [15.464 sec/step, loss=3.18659, avg_loss=4.98184]
[2025-11-02 18:49:31.437]    Step 15      [15.625 sec/step, loss=4.83451, avg_loss=4.97201]
[2025-11-02 18:49:40.785]    Step 16      [15.232 sec/step, loss=3.00502, avg_loss=4.84908]
[2025-11-02 18:49:50.278]    Step 17      [14.895 sec/step, loss=2.89989, avg_loss=4.73442]
[2025-11-02 18:50:07.844]    Step 18      [15.043 sec/step, loss=3.77202, avg_loss=4.68095]
[2025-11-02 18:50:17.056]    Step 19      [14.736 sec/step, loss=2.77880, avg_loss=4.58084]
[2025-11-02 18:50:26.243]    Step 20      [14.459 sec/step, loss=2.72128, avg_loss=4.48786]
[2025-11-02 18:51:02.213]    Step 21      [15.483 sec/step, loss=3.65831, avg_loss=4.44836]
[2025-11-02 18:51:11.539]    Step 22      [15.203 sec/step, loss=4.00119, avg_loss=4.42803]
[2025-11-02 18:51:21.101]    Step 23      [14.958 sec/step, loss=2.62573, avg_loss=4.34967]
[2025-11-02 18:51:36.046]    Generated 32 batches of size 32 in 14.798 sec
[2025-11-02 18:52:15.728]    Step 24      [16.611 sec/step, loss=3.76377, avg_loss=4.32526]
[2025-11-02 18:53:04.591]    Step 25      [17.901 sec/step, loss=3.56083, avg_loss=4.29468]
[2025-11-02 18:53:15.097]    Step 26      [17.617 sec/step, loss=2.61908, avg_loss=4.23024]
[2025-11-02 18:53:37.173]    Step 27      [17.782 sec/step, loss=3.13181, avg_loss=4.18955]
[2025-11-02 18:53:47.767]    Step 28      [17.525 sec/step, loss=2.52694, avg_loss=4.13017]
[2025-11-02 18:54:43.238]    Step 29      [18.834 sec/step, loss=3.31540, avg_loss=4.10208]
[2025-11-02 18:54:52.437]    Step 30      [18.512 sec/step, loss=3.38075, avg_loss=4.07803]
[2025-11-02 18:55:12.045]    Step 31      [18.548 sec/step, loss=3.10046, avg_loss=4.04650]
[2025-11-02 18:55:22.447]    Step 32      [18.293 sec/step, loss=2.67167, avg_loss=4.00354]
[2025-11-02 18:55:41.425]    Step 33      [18.314 sec/step, loss=3.13755, avg_loss=3.97729]
[2025-11-02 18:55:51.560]    Step 34      [18.073 sec/step, loss=2.46078, avg_loss=3.93269]
[2025-11-02 18:56:49.061]    Step 35      [19.200 sec/step, loss=3.13336, avg_loss=3.90985]
[2025-11-02 18:57:10.526]    Step 36      [19.263 sec/step, loss=3.02157, avg_loss=3.88518]
[2025-11-02 18:57:20.963]    Step 37      [19.024 sec/step, loss=2.26669, avg_loss=3.84144]
[2025-11-02 18:57:40.981]    Step 38      [19.050 sec/step, loss=2.87617, avg_loss=3.81603]
[2025-11-02 18:58:30.617]    Step 39      [19.835 sec/step, loss=2.88141, avg_loss=3.79207]
[2025-11-02 18:58:40.810]    Step 40      [19.594 sec/step, loss=2.17791, avg_loss=3.75171]
[2025-11-02 18:58:50.635]    Step 41      [19.355 sec/step, loss=2.12325, avg_loss=3.71200]
[2025-11-02 18:59:28.194]    Step 42      [19.789 sec/step, loss=2.81166, avg_loss=3.69056]
[2025-11-02 18:59:36.606]    Step 43      [19.524 sec/step, loss=2.87342, avg_loss=3.67156]
[2025-11-02 18:59:46.513]    Step 44      [19.306 sec/step, loss=2.06263, avg_loss=3.63499]
[2025-11-02 18:59:56.303]    Step 45      [19.094 sec/step, loss=2.55923, avg_loss=3.61108]
[2025-11-02 19:00:06.205]    Step 46      [18.894 sec/step, loss=2.02909, avg_loss=3.57669]
[2025-11-02 19:00:45.067]    Step 47      [19.319 sec/step, loss=2.66232, avg_loss=3.55724]
[2025-11-02 19:00:55.551]    Step 48      [19.135 sec/step, loss=1.94654, avg_loss=3.52368]
[2025-11-02 19:01:15.516]    Step 49      [19.152 sec/step, loss=2.53339, avg_loss=3.50347]
[2025-11-02 19:01:24.057]    Step 50      [18.940 sec/step, loss=2.52239, avg_loss=3.48385]
[2025-11-02 19:01:33.614]    Step 51      [18.756 sec/step, loss=1.99452, avg_loss=3.45465]
[2025-11-02 19:01:43.219]    Step 52      [18.580 sec/step, loss=1.94752, avg_loss=3.42566]
[2025-11-02 19:02:02.696]    Step 53      [18.597 sec/step, loss=2.36190, avg_loss=3.40559]
[2025-11-02 19:02:20.275]    Step 54      [18.578 sec/step, loss=2.27181, avg_loss=3.38460]
[2025-11-02 19:03:09.409]    Step 55      [19.133 sec/step, loss=2.44164, avg_loss=3.36745]
[2025-11-02 19:03:21.577]    Generated 32 batches of size 32 in 12.017 sec
[2025-11-02 19:03:29.686]    Step 56      [19.154 sec/step, loss=2.28774, avg_loss=3.34817]
[2025-11-02 19:03:39.203]    Step 57      [18.985 sec/step, loss=1.80834, avg_loss=3.32116]
[2025-11-02 19:03:48.806]    Step 58      [18.823 sec/step, loss=1.79942, avg_loss=3.29492]
[2025-11-02 19:03:58.383]    Step 59      [18.666 sec/step, loss=1.77693, avg_loss=3.26919]
[2025-11-02 19:04:08.071]    Step 60      [18.517 sec/step, loss=1.70912, avg_loss=3.24319]
[2025-11-02 19:04:17.511]    Step 61      [18.368 sec/step, loss=1.66388, avg_loss=3.21730]
[2025-11-02 19:04:37.537]    Step 62      [18.395 sec/step, loss=2.18775, avg_loss=3.20070]
[2025-11-02 19:05:17.004]    Step 63      [18.729 sec/step, loss=2.25292, avg_loss=3.18565]
[2025-11-02 19:05:26.475]    Step 64      [18.584 sec/step, loss=1.66179, avg_loss=3.16184]
[2025-11-02 19:05:36.052]    Step 65      [18.446 sec/step, loss=1.59641, avg_loss=3.13776]
[2025-11-02 19:05:45.686]    Step 66      [18.312 sec/step, loss=1.54655, avg_loss=3.11365]
[2025-11-02 19:05:54.055]    Step 67      [18.164 sec/step, loss=2.14690, avg_loss=3.09922]
[2025-11-02 19:06:03.469]    Step 68      [18.035 sec/step, loss=1.56576, avg_loss=3.07667]
[2025-11-02 19:06:11.762]    Step 69      [17.894 sec/step, loss=2.00578, avg_loss=3.06115]
[2025-11-02 19:06:29.731]    Step 70      [17.895 sec/step, loss=1.93114, avg_loss=3.04500]
[2025-11-02 19:06:39.329]    Step 71      [17.778 sec/step, loss=1.52234, avg_loss=3.02356]
[2025-11-02 19:06:48.782]    Step 72      [17.663 sec/step, loss=1.48979, avg_loss=3.00226]
[2025-11-02 19:07:05.748]    Step 73      [17.653 sec/step, loss=1.97626, avg_loss=2.98820]
[2025-11-02 19:07:22.781]    Step 74      [17.645 sec/step, loss=1.69113, avg_loss=2.97067]
[2025-11-02 19:07:32.031]    Step 75      [17.533 sec/step, loss=1.48565, avg_loss=2.95087]
[2025-11-02 19:07:40.907]    Step 76      [17.419 sec/step, loss=1.43262, avg_loss=2.93090]
[2025-11-02 19:07:49.701]    Step 77      [17.307 sec/step, loss=1.39639, avg_loss=2.91097]
[2025-11-02 19:08:25.879]    Step 78      [17.549 sec/step, loss=2.12833, avg_loss=2.90093]
[2025-11-02 19:08:44.812]    Step 79      [17.566 sec/step, loss=1.93141, avg_loss=2.88866]
[2025-11-02 19:09:01.560]    Step 80      [17.556 sec/step, loss=1.78178, avg_loss=2.87483]
[2025-11-02 19:09:10.236]    Step 81      [17.446 sec/step, loss=1.44348, avg_loss=2.85715]
[2025-11-02 19:09:46.769]    Step 82      [17.679 sec/step, loss=1.95349, avg_loss=2.84613]
[2025-11-02 19:09:55.805]    Step 83      [17.575 sec/step, loss=1.37639, avg_loss=2.82843]
[2025-11-02 19:10:14.748]    Step 84      [17.591 sec/step, loss=1.79078, avg_loss=2.81607]
[2025-11-02 19:10:23.631]    Step 85      [17.489 sec/step, loss=1.34637, avg_loss=2.79878]
[2025-11-02 19:10:42.419]    Step 86      [17.504 sec/step, loss=1.73746, avg_loss=2.78644]
[2025-11-02 19:11:03.167]    Step 87      [17.541 sec/step, loss=1.71026, avg_loss=2.77407]
[2025-11-02 19:11:12.381]    Generated 32 batches of size 32 in 9.064 sec
[2025-11-02 19:11:13.676]    Step 88      [17.461 sec/step, loss=1.33010, avg_loss=2.75766]
[2025-11-02 19:11:53.726]    Step 89      [17.715 sec/step, loss=1.80378, avg_loss=2.74695]
[2025-11-02 19:12:03.071]    Step 90      [17.622 sec/step, loss=1.27417, avg_loss=2.73058]
[2025-11-02 19:12:12.403]    Step 91      [17.531 sec/step, loss=1.59935, avg_loss=2.71815]
[2025-11-02 19:12:21.535]    Step 92      [17.440 sec/step, loss=1.23666, avg_loss=2.70205]
[2025-11-02 19:12:30.670]    Step 93      [17.350 sec/step, loss=1.22126, avg_loss=2.68612]
[2025-11-02 19:13:21.375]    Step 94      [17.705 sec/step, loss=1.73833, avg_loss=2.67604]
[2025-11-02 19:14:08.054]    Step 95      [18.010 sec/step, loss=1.74223, avg_loss=2.66621]
[2025-11-02 19:14:55.550]    Step 96      [18.317 sec/step, loss=1.72081, avg_loss=2.65636]
[2025-11-02 19:15:04.920]    Step 97      [18.225 sec/step, loss=1.20838, avg_loss=2.64144]
[2025-11-02 19:15:44.158]    Step 98      [18.440 sec/step, loss=1.68401, avg_loss=2.63167]
[2025-11-02 19:15:52.159]    Step 99      [18.334 sec/step, loss=1.57119, avg_loss=2.62095]
[2025-11-02 19:16:01.353]    Step 100     [18.243 sec/step, loss=1.20420, avg_loss=2.60679]
[2025-11-02 19:16:01.353]    Writing summary at step: 100
[2025-11-02 19:16:44.762]    Step 101     [18.313 sec/step, loss=1.53568, avg_loss=2.55019]
[2025-11-02 19:17:24.367]    Step 102     [18.542 sec/step, loss=1.48334, avg_loss=2.50511]
[2025-11-02 19:17:34.592]    Step 103     [18.555 sec/step, loss=1.17332, avg_loss=2.45766]
[2025-11-02 19:17:44.690]    Step 104     [18.318 sec/step, loss=1.14561, avg_loss=2.39859]
[2025-11-02 19:17:54.665]    Step 105     [18.231 sec/step, loss=1.12025, avg_loss=2.34875]
[2025-11-02 19:18:04.870]    Step 106     [18.242 sec/step, loss=1.11811, avg_loss=2.31293]
[2025-11-02 19:18:22.489]    Step 107     [18.327 sec/step, loss=1.25347, avg_loss=2.28303]
[2025-11-02 19:18:40.735]    Step 108     [18.340 sec/step, loss=1.52588, avg_loss=2.25185]
[2025-11-02 19:19:18.983]    Step 109     [18.643 sec/step, loss=1.57425, avg_loss=2.21182]
[2025-11-02 19:20:09.170]    Step 110     [18.793 sec/step, loss=1.59496, avg_loss=2.18610]
[2025-11-02 19:20:19.621]    Step 111     [18.801 sec/step, loss=1.12926, avg_loss=2.15974]
[2025-11-02 19:20:29.923]    Step 112     [18.711 sec/step, loss=1.10641, avg_loss=2.13236]
[2025-11-02 19:20:40.744]    Step 113     [18.725 sec/step, loss=1.06938, avg_loss=2.10950]
[2025-11-02 19:21:01.018]    Step 114     [18.836 sec/step, loss=1.55361, avg_loss=2.09317]
[2025-11-02 19:21:11.512]    Step 115     [18.763 sec/step, loss=1.05039, avg_loss=2.05532]
[2025-11-02 19:22:03.149]    Step 116     [19.186 sec/step, loss=1.55774, avg_loss=2.04085]
[2025-11-02 19:22:23.493]    Step 117     [19.294 sec/step, loss=1.45839, avg_loss=2.02644]
[2025-11-02 19:22:31.874]    Step 118     [19.202 sec/step, loss=1.49281, avg_loss=2.00364]
[2025-11-02 19:22:46.133]    Generated 32 batches of size 32 in 14.090 sec
[2025-11-02 19:23:22.975]    Step 119     [19.621 sec/step, loss=1.53183, avg_loss=1.99117]
[2025-11-02 19:23:32.421]    Step 120     [19.624 sec/step, loss=1.25816, avg_loss=1.97654]
[2025-11-02 19:23:41.756]    Step 121     [19.357 sec/step, loss=1.03158, avg_loss=1.95028]
[2025-11-02 19:23:59.125]    Step 122     [19.438 sec/step, loss=1.39985, avg_loss=1.92426]
[2025-11-02 19:24:08.633]    Step 123     [19.437 sec/step, loss=1.00943, avg_loss=1.90810]
[2025-11-02 19:24:18.036]    Step 124     [18.985 sec/step, loss=0.99932, avg_loss=1.88045]
[2025-11-02 19:24:27.550]    Step 125     [18.591 sec/step, loss=0.98595, avg_loss=1.85471]
[2025-11-02 19:24:45.289]    Step 126     [18.664 sec/step, loss=1.53728, avg_loss=1.84389]
[2025-11-02 19:24:54.714]    Step 127     [18.537 sec/step, loss=0.97735, avg_loss=1.82234]
[2025-11-02 19:25:03.346]    Step 128     [18.518 sec/step, loss=1.36207, avg_loss=1.81069]
[2025-11-02 19:25:12.807]    Step 129     [18.058 sec/step, loss=0.96738, avg_loss=1.78721]
[2025-11-02 19:25:32.332]    Step 130     [18.161 sec/step, loss=1.42451, avg_loss=1.76765]
[2025-11-02 19:25:41.790]    Step 131     [18.059 sec/step, loss=0.95553, avg_loss=1.74620]
[2025-11-02 19:25:51.247]    Step 132     [18.050 sec/step, loss=0.93686, avg_loss=1.72885]
[2025-11-02 19:26:00.667]    Step 133     [17.954 sec/step, loss=0.91665, avg_loss=1.70665]
[2025-11-02 19:26:18.266]    Step 134     [18.029 sec/step, loss=1.27556, avg_loss=1.69479]
[2025-11-02 19:26:54.622]    Step 135     [17.817 sec/step, loss=1.20535, avg_loss=1.67551]
[2025-11-02 19:27:03.615]    Step 136     [17.693 sec/step, loss=0.93222, avg_loss=1.65462]
[2025-11-02 19:27:12.688]    Step 137     [17.679 sec/step, loss=0.90827, avg_loss=1.64104]
[2025-11-02 19:28:00.153]    Step 138     [17.954 sec/step, loss=1.42594, avg_loss=1.62653]
[2025-11-02 19:28:09.648]    Step 139     [17.552 sec/step, loss=0.88605, avg_loss=1.60658]
[2025-11-02 19:29:02.001]    Step 140     [17.974 sec/step, loss=1.38000, avg_loss=1.59860]
[2025-11-02 19:29:37.932]    Step 141     [18.235 sec/step, loss=1.40243, avg_loss=1.59139]
[2025-11-02 19:29:46.144]    Step 142     [17.941 sec/step, loss=1.25696, avg_loss=1.57585]
[2025-11-02 19:29:55.371]    Step 143     [17.949 sec/step, loss=1.07104, avg_loss=1.55782]
[2025-11-02 19:30:04.971]    Step 144     [17.946 sec/step, loss=0.89350, avg_loss=1.54613]
[2025-11-02 19:30:41.082]    Step 145     [18.210 sec/step, loss=1.45933, avg_loss=1.53513]
[2025-11-02 19:30:50.533]    Step 146     [18.205 sec/step, loss=0.86141, avg_loss=1.52345]
[2025-11-02 19:31:07.547]    Step 147     [17.987 sec/step, loss=1.34590, avg_loss=1.51029]
[2025-11-02 19:31:56.025]    Step 148     [18.367 sec/step, loss=1.29574, avg_loss=1.50378]
[2025-11-02 19:32:04.945]    Step 149     [18.256 sec/step, loss=0.86072, avg_loss=1.48706]
[2025-11-02 19:32:13.968]    Step 150     [18.261 sec/step, loss=0.85674, avg_loss=1.47040]
[2025-11-02 19:32:24.544]    Generated 32 batches of size 32 in 10.418 sec
[2025-11-02 19:32:32.555]    Step 151     [18.351 sec/step, loss=1.23723, avg_loss=1.46283]
[2025-11-02 19:32:49.830]    Step 152     [18.428 sec/step, loss=1.29310, avg_loss=1.45628]
[2025-11-02 19:32:58.659]    Step 153     [18.321 sec/step, loss=0.88236, avg_loss=1.44149]
[2025-11-02 19:33:17.041]    Step 154     [18.329 sec/step, loss=1.33882, avg_loss=1.43216]
[2025-11-02 19:33:26.092]    Step 155     [17.929 sec/step, loss=0.84278, avg_loss=1.41617]
[2025-11-02 19:33:35.084]    Step 156     [17.816 sec/step, loss=0.80864, avg_loss=1.40138]
[2025-11-02 19:33:53.380]    Step 157     [17.904 sec/step, loss=1.29854, avg_loss=1.39628]
[2025-11-02 19:34:02.828]    Step 158     [17.902 sec/step, loss=0.83533, avg_loss=1.38664]
[2025-11-02 19:34:19.495]    Step 159     [17.973 sec/step, loss=0.87567, avg_loss=1.37763]
[2025-11-02 19:34:38.003]    Step 160     [18.061 sec/step, loss=1.23822, avg_loss=1.37292]
[2025-11-02 19:34:47.040]    Step 161     [18.057 sec/step, loss=0.79583, avg_loss=1.36424]
[2025-11-02 19:34:56.284]    Step 162     [17.949 sec/step, loss=0.78027, avg_loss=1.35016]
[2025-11-02 19:35:14.506]    Step 163     [17.737 sec/step, loss=1.24529, avg_loss=1.34009]
[2025-11-02 19:35:22.356]    Step 164     [17.721 sec/step, loss=1.33762, avg_loss=1.33684]
[2025-11-02 19:35:30.210]    Step 165     [17.703 sec/step, loss=1.21904, avg_loss=1.33307]
[2025-11-02 19:35:39.171]    Step 166     [17.697 sec/step, loss=0.82542, avg_loss=1.32586]
[2025-11-02 19:35:48.043]    Step 167     [17.702 sec/step, loss=0.81320, avg_loss=1.31252]
[2025-11-02 19:35:56.954]    Exiting due to exception: [Errno 22] Invalid argument

-----------------------------------------------------------------
Starting new training run
-----------------------------------------------------------------
[2025-11-02 22:29:22.540]    ==================================================
[2025-11-02 22:29:22.540]    ==================================================
[2025-11-02 22:29:22.540]     [*] Checkpoint path: logdir-tacotron2/moon+son_2025-11-02_18-45-07\model.ckpt
[2025-11-02 22:29:22.540]     [*] Loading training data from: ['.\\data\\moon', '.\\data\\son']
[2025-11-02 22:29:22.540]     [*] Using model: logdir-tacotron2/moon+son_2025-11-02_18-45-07
[2025-11-02 22:29:22.541]    Hyperparameters:
  adam_beta1: 0.9
  adam_beta2: 0.999
  allow_clipping_in_normalization: True
  attention_dim: 128
  attention_filters: 32
  attention_kernel: [31]
  attention_size: 128
  attention_type: bah_mon_norm
  attention_win_size: 7
  cleaners: korean_cleaners
  clip_mels_length: True
  cumulative_weights: True
  dec_prenet_sizes: [256, 256]
  decoder_layers: 2
  decoder_lstm_units: 1024
  dilation_channels: 256
  dilations: [1, 2, 4, 8, 16, 32, 64, 128, 256, 512, 1, 2, 4, 8, 16, 32, 64, 128, 256, 512]
  dropout_prob: 0.5
  embedding_size: 512
  enc_conv_channels: 512
  enc_conv_kernel_size: 5
  enc_conv_num_layers: 3
  encoder_lstm_units: 256
  fft_size: 2048
  filter_width: 3
  gc_channels: 32
  griffin_lim_iters: 60
  hop_size: 300
  inference_prenet_dropout: True
  initial_data_greedy: True
  initial_phase_step: 8000
  input_type: raw
  l2_regularization_strength: 0
  legacy: True
  main_data: ['']
  main_data_greedy_factor: 0
  mask_encoder: True
  max_abs_value: 4.0
  max_checkpoints: 3
  max_mel_frames: 1000
  max_n_frame: 1000
  min_level_db: -100
  min_n_frame: 150
  min_tokens: 30
  model_type: multi-speaker
  momentum: 0.9
  name: Tacotron-2
  num_mels: 80
  num_steps: 1000000
  optimizer: adam
  out_channels: 30
  post_bank_channel_size: 128
  post_bank_size: 8
  post_highway_depth: 4
  post_maxpool_width: 2
  post_proj_sizes: [256, 80]
  post_proj_width: 3
  post_rnn_size: 128
  postnet_channels: 512
  postnet_kernel_size: [5]
  postnet_num_layers: 5
  power: 1.5
  preemphasis: 0.97
  preemphasize: True
  prenet_layers: [256, 256]
  prioritize_loss: False
  quantization_channels: 256
  reduction_factor: 2
  ref_level_db: 20
  rescaling: True
  rescaling_max: 0.999
  residual_channels: 128
  residual_legacy: True
  sample_rate: 24000
  sample_size: 9000
  scalar_input: True
  signal_normalization: True
  silence_threshold: 0
  skip_channels: 128
  skip_inadequate: False
  skip_path_filter: False
  smoothing: False
  speaker_embedding_size: 16
  store_metadata: False
  symmetric_mels: True
  synthesis_constraint: False
  synthesis_constraint_type: window
  tacotron_decay_learning_rate: True
  tacotron_decay_rate: 0.5
  tacotron_decay_steps: 18000
  tacotron_final_learning_rate: 0.0001
  tacotron_initial_learning_rate: 0.001
  tacotron_reg_weight: 1e-06
  tacotron_start_decay: 40000
  tacotron_zoneout_rate: 0.1
  trim_fft_size: 512
  trim_hop_size: 128
  trim_silence: True
  trim_top_db: 23
  upsample_factor: [12, 25]
  upsample_type: SubPixel
  use_biases: True
  use_lws: False
  wavenet_batch_size: 2
  wavenet_clip_gradients: True
  wavenet_decay_rate: 0.5
  wavenet_decay_steps: 300000
  wavenet_dropout: 0.05
  wavenet_learning_rate: 0.001
  win_size: 1200
[2025-11-02 22:29:26.550]     [.\data\moon] Loaded metadata for 34 examples (0.02 hours)
[2025-11-02 22:29:26.550]     [.\data\moon] Max length: 340
[2025-11-02 22:29:26.550]     [.\data\moon] Min length: 152
[2025-11-02 22:29:30.708]     [.\data\son] Loaded metadata for 38 examples (0.05 hours)
[2025-11-02 22:29:30.708]     [.\data\son] Max length: 827
[2025-11-02 22:29:30.708]     [.\data\son] Min length: 176
[2025-11-02 22:29:30.708]    ========================================
[2025-11-02 22:29:30.708]    Data Amount:
[2025-11-02 22:29:30.708]    {'.\\data\\moon': 0.5, '.\\data\\son': 0.5}
[2025-11-02 22:29:30.709]    ========================================
[2025-11-02 22:29:34.905]     [.\data\moon] Loaded metadata for 34 examples (0.02 hours)
[2025-11-02 22:29:34.905]     [.\data\moon] Max length: 340
[2025-11-02 22:29:34.905]     [.\data\moon] Min length: 152
[2025-11-02 22:29:39.124]     [.\data\son] Loaded metadata for 38 examples (0.05 hours)
[2025-11-02 22:29:39.124]     [.\data\son] Max length: 827
[2025-11-02 22:29:39.124]     [.\data\son] Min length: 176
[2025-11-02 22:29:39.124]    ========================================
[2025-11-02 22:29:39.124]    Data Amount:
[2025-11-02 22:29:39.124]    {'.\\data\\moon': 0.5, '.\\data\\son': 0.5}
[2025-11-02 22:29:39.124]    ========================================
[2025-11-02 22:29:41.365]    ========================================
[2025-11-02 22:29:41.365]     model_type: multi-speaker
[2025-11-02 22:29:41.365]    ========================================
[2025-11-02 22:29:41.365]    Initialized Tacotron model. Dimensions: 
[2025-11-02 22:29:41.365]        embedding:                512
[2025-11-02 22:29:41.365]        encoder conv out:               512
[2025-11-02 22:29:41.365]        encoder out:              512
[2025-11-02 22:29:41.365]        attention out:            1024
[2025-11-02 22:29:41.365]        decoder prenet lstm concat out :        1536
[2025-11-02 22:29:41.365]        decoder cell out:         162
[2025-11-02 22:29:41.365]        decoder out (2 frames):  162
[2025-11-02 22:29:41.365]        decoder mel out:    80
[2025-11-02 22:29:41.365]        mel out:    80
[2025-11-02 22:29:41.365]        postnet out:              256
[2025-11-02 22:29:41.365]        linear out:               1025
[2025-11-02 22:29:41.365]      Tacotron Parameters       29.229 Million.
[2025-11-02 22:29:48.299]    ========================================
[2025-11-02 22:29:48.299]     model_type: multi-speaker
[2025-11-02 22:29:48.299]    ========================================
[2025-11-02 22:29:48.299]    Initialized Tacotron model. Dimensions: 
[2025-11-02 22:29:48.299]        embedding:                512
[2025-11-02 22:29:48.299]        encoder conv out:               512
[2025-11-02 22:29:48.299]        encoder out:              512
[2025-11-02 22:29:48.299]        attention out:            1024
[2025-11-02 22:29:48.299]        decoder prenet lstm concat out :        1536
[2025-11-02 22:29:48.299]        decoder cell out:         162
[2025-11-02 22:29:48.299]        decoder out (2 frames):  162
[2025-11-02 22:29:48.299]        decoder mel out:    80
[2025-11-02 22:29:48.299]        mel out:    80
[2025-11-02 22:29:48.299]        postnet out:              256
[2025-11-02 22:29:48.299]        linear out:               1025
[2025-11-02 22:29:48.299]      Tacotron Parameters       29.229 Million.
[2025-11-02 22:30:08.709]    Exiting due to exception: max() arg is an empty sequence

-----------------------------------------------------------------
Starting new training run
-----------------------------------------------------------------
[2025-11-03 02:05:55.795]    ==================================================
[2025-11-03 02:05:55.795]    ==================================================
[2025-11-03 02:05:55.795]     [*] Checkpoint path: logdir-tacotron2/moon+son_2025-11-02_18-45-07\model.ckpt
[2025-11-03 02:05:55.795]     [*] Loading training data from: ['.\\data\\moon', '.\\data\\son']
[2025-11-03 02:05:55.795]     [*] Using model: logdir-tacotron2/moon+son_2025-11-02_18-45-07
[2025-11-03 02:05:55.795]    Hyperparameters:
  adam_beta1: 0.9
  adam_beta2: 0.999
  allow_clipping_in_normalization: True
  attention_dim: 128
  attention_filters: 32
  attention_kernel: [31]
  attention_size: 128
  attention_type: bah_mon_norm
  attention_win_size: 7
  cleaners: korean_cleaners
  clip_mels_length: True
  cumulative_weights: True
  dec_prenet_sizes: [256, 256]
  decoder_layers: 2
  decoder_lstm_units: 1024
  dilation_channels: 256
  dilations: [1, 2, 4, 8, 16, 32, 64, 128, 256, 512, 1, 2, 4, 8, 16, 32, 64, 128, 256, 512]
  dropout_prob: 0.5
  embedding_size: 512
  enc_conv_channels: 512
  enc_conv_kernel_size: 5
  enc_conv_num_layers: 3
  encoder_lstm_units: 256
  fft_size: 2048
  filter_width: 3
  gc_channels: 32
  griffin_lim_iters: 60
  hop_size: 300
  inference_prenet_dropout: True
  initial_data_greedy: True
  initial_phase_step: 8000
  input_type: raw
  l2_regularization_strength: 0
  legacy: True
  main_data: ['']
  main_data_greedy_factor: 0
  mask_encoder: True
  max_abs_value: 4.0
  max_checkpoints: 3
  max_mel_frames: 1000
  max_n_frame: 1000
  min_level_db: -100
  min_n_frame: 150
  min_tokens: 30
  model_type: multi-speaker
  momentum: 0.9
  name: Tacotron-2
  num_mels: 80
  num_steps: 1000000
  optimizer: adam
  out_channels: 30
  post_bank_channel_size: 128
  post_bank_size: 8
  post_highway_depth: 4
  post_maxpool_width: 2
  post_proj_sizes: [256, 80]
  post_proj_width: 3
  post_rnn_size: 128
  postnet_channels: 512
  postnet_kernel_size: [5]
  postnet_num_layers: 5
  power: 1.5
  preemphasis: 0.97
  preemphasize: True
  prenet_layers: [256, 256]
  prioritize_loss: False
  quantization_channels: 256
  reduction_factor: 2
  ref_level_db: 20
  rescaling: True
  rescaling_max: 0.999
  residual_channels: 128
  residual_legacy: True
  sample_rate: 24000
  sample_size: 9000
  scalar_input: True
  signal_normalization: True
  silence_threshold: 0
  skip_channels: 128
  skip_inadequate: False
  skip_path_filter: False
  smoothing: False
  speaker_embedding_size: 16
  store_metadata: False
  symmetric_mels: True
  synthesis_constraint: False
  synthesis_constraint_type: window
  tacotron_decay_learning_rate: True
  tacotron_decay_rate: 0.5
  tacotron_decay_steps: 18000
  tacotron_final_learning_rate: 0.0001
  tacotron_initial_learning_rate: 0.001
  tacotron_reg_weight: 1e-06
  tacotron_start_decay: 40000
  tacotron_zoneout_rate: 0.1
  trim_fft_size: 512
  trim_hop_size: 128
  trim_silence: True
  trim_top_db: 23
  upsample_factor: [12, 25]
  upsample_type: SubPixel
  use_biases: True
  use_lws: False
  wavenet_batch_size: 2
  wavenet_clip_gradients: True
  wavenet_decay_rate: 0.5
  wavenet_decay_steps: 300000
  wavenet_dropout: 0.05
  wavenet_learning_rate: 0.001
  win_size: 1200
[2025-11-03 02:06:00.282]     [.\data\moon] Loaded metadata for 34 examples (0.02 hours)
[2025-11-03 02:06:00.282]     [.\data\moon] Max length: 340
[2025-11-03 02:06:00.282]     [.\data\moon] Min length: 152
[2025-11-03 02:06:04.637]     [.\data\son] Loaded metadata for 38 examples (0.05 hours)
[2025-11-03 02:06:04.637]     [.\data\son] Max length: 827
[2025-11-03 02:06:04.637]     [.\data\son] Min length: 176
[2025-11-03 02:06:04.637]    ========================================
[2025-11-03 02:06:04.637]    Data Amount:
[2025-11-03 02:06:04.637]    {'.\\data\\moon': 0.5, '.\\data\\son': 0.5}
[2025-11-03 02:06:04.637]    ========================================
[2025-11-03 02:06:09.082]     [.\data\moon] Loaded metadata for 34 examples (0.02 hours)
[2025-11-03 02:06:09.082]     [.\data\moon] Max length: 340
[2025-11-03 02:06:09.082]     [.\data\moon] Min length: 152
[2025-11-03 02:06:13.687]     [.\data\son] Loaded metadata for 38 examples (0.05 hours)
[2025-11-03 02:06:13.687]     [.\data\son] Max length: 827
[2025-11-03 02:06:13.687]     [.\data\son] Min length: 176
[2025-11-03 02:06:13.687]    ========================================
[2025-11-03 02:06:13.687]    Data Amount:
[2025-11-03 02:06:13.687]    {'.\\data\\moon': 0.5, '.\\data\\son': 0.5}
[2025-11-03 02:06:13.687]    ========================================
[2025-11-03 02:06:15.248]    ========================================
[2025-11-03 02:06:15.248]     model_type: multi-speaker
[2025-11-03 02:06:15.248]    ========================================
[2025-11-03 02:06:15.248]    Initialized Tacotron model. Dimensions: 
[2025-11-03 02:06:15.248]        embedding:                512
[2025-11-03 02:06:15.248]        encoder conv out:               512
[2025-11-03 02:06:15.248]        encoder out:              512
[2025-11-03 02:06:15.248]        attention out:            1024
[2025-11-03 02:06:15.248]        decoder prenet lstm concat out :        1536
[2025-11-03 02:06:15.248]        decoder cell out:         162
[2025-11-03 02:06:15.248]        decoder out (2 frames):  162
[2025-11-03 02:06:15.248]        decoder mel out:    80
[2025-11-03 02:06:15.248]        mel out:    80
[2025-11-03 02:06:15.248]        postnet out:              256
[2025-11-03 02:06:15.248]        linear out:               1025
[2025-11-03 02:06:15.248]      Tacotron Parameters       29.229 Million.
[2025-11-03 02:06:19.403]    ========================================
[2025-11-03 02:06:19.403]     model_type: multi-speaker
[2025-11-03 02:06:19.403]    ========================================
[2025-11-03 02:06:19.403]    Initialized Tacotron model. Dimensions: 
[2025-11-03 02:06:19.403]        embedding:                512
[2025-11-03 02:06:19.403]        encoder conv out:               512
[2025-11-03 02:06:19.403]        encoder out:              512
[2025-11-03 02:06:19.403]        attention out:            1024
[2025-11-03 02:06:19.403]        decoder prenet lstm concat out :        1536
[2025-11-03 02:06:19.403]        decoder cell out:         162
[2025-11-03 02:06:19.403]        decoder out (2 frames):  162
[2025-11-03 02:06:19.403]        decoder mel out:    80
[2025-11-03 02:06:19.404]        mel out:    80
[2025-11-03 02:06:19.404]        postnet out:              256
[2025-11-03 02:06:19.404]        linear out:               1025
[2025-11-03 02:06:19.404]      Tacotron Parameters       29.229 Million.
[2025-11-03 02:06:19.603]    Checking TensorFlow version...
[2025-11-03 02:06:19.603]    TensorFlow version: 1.15.0
[2025-11-03 02:06:19.603]    Checking GPU availability...
[2025-11-03 02:06:19.626]    All devices found:
[2025-11-03 02:06:19.626]      Device: /device:CPU:0 (Type: CPU, Memory: 268435456)
[2025-11-03 02:06:19.626]    ERROR: No GPU devices found! Training requires GPU.
[2025-11-03 02:06:19.626]    This might be due to:
[2025-11-03 02:06:19.626]      1. CUDA/cuDNN not properly installed
[2025-11-03 02:06:19.626]      2. GPU drivers not installed
[2025-11-03 02:06:19.626]      3. TensorFlow GPU version not installed
[2025-11-03 02:06:19.626]    ERROR checking GPU availability
[2025-11-03 02:06:19.626]    Error type: Exception
[2025-11-03 02:06:19.626]    Error message: No GPU devices available. Cannot proceed with GPU-only training.
[2025-11-03 02:06:19.626]    Error details: Exception('No GPU devices available. Cannot proceed with GPU-only training.')
[2025-11-03 02:06:19.626]    Traceback:
Traceback (most recent call last):
  File "train_tacotron2.py", line 231, in train
    raise Exception('No GPU devices available. Cannot proceed with GPU-only training.')
Exception: No GPU devices available. Cannot proceed with GPU-only training.

